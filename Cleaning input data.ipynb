{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data= fetch_openml('mnist_784', version=1)#Get data from https://www.openml.org/d/554\n",
    "dfData = pd.DataFrame(np.c_[data[\"data\"],data[\"target\"]],columns = data[\"feature_names\"]+[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making about part of our data null\n",
    "fivePer = int(0.05*dfData.shape[0])\n",
    "allInds = np.arange(0,dfData.shape[0],1)\n",
    "for col in dfData:\n",
    "    if col == \"target\":\n",
    "        continue\n",
    "    #get at most 5% unique indeies and set those values to null\n",
    "    indsToNull = np.unique(np.random.choice(allInds,replace=True,size=fivePer))\n",
    "    dfData[col].iloc[indsToNull] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null containing rows: 70000\n",
      "Number of null containing columns: 784\n",
      "Data shape: (70000, 785)\n"
     ]
    }
   ],
   "source": [
    "#High number of rows and columns means it's very likely every row and column contains null values\n",
    "print(\"Number of null containing rows:\",pd.isnull(dfData.astype(float).sum(axis=1,skipna=False)).sum())\n",
    "print(\"Number of null containing columns:\",pd.isnull(dfData.astype(float).sum(axis=0,skipna=False)).sum())\n",
    "print(\"Data shape:\",dfData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to train with null values\n",
      "Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Can't train with null values\n"
     ]
    }
   ],
   "source": [
    "stratSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in stratSplit.split(dfData[data[\"feature_names\"]], dfData[\"target\"]):\n",
    "    X_train = dfData[data[\"feature_names\"]].iloc[train_index].reset_index(drop=True)\n",
    "    X_test = dfData[data[\"feature_names\"]].iloc[test_index].reset_index(drop=True)\n",
    "    \n",
    "    y_train = dfData[\"target\"].iloc[train_index].reset_index(drop=True)\n",
    "    y_test = dfData[\"target\"].iloc[test_index].reset_index(drop=True)\n",
    "    \n",
    "log_reg = LogisticRegression()\n",
    "print(\"Trying to train with null values\")\n",
    "try:\n",
    "    log_reg.fit(X_train,y_train)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Can't train with null values\")\n",
    "# print(\"Accuracy with null values in data:\",log_reg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in missing data using subject/domain knowledge, or going back to the source to see if missing data\n",
    "#can still be retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (56000, 784)\n",
      "After: (36656, 784)\n",
      "Before: (56000, 784)\n",
      "After: (55996, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxschallwig/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with dropped rows with null values: 0.9145653260947211\n"
     ]
    }
   ],
   "source": [
    "#drop rows with null values\n",
    "#tolerate at most 40 null values in a row\n",
    "numColumns = X_train.shape[1]\n",
    "X_trainDroppedRows = X_train.dropna(axis=0,how=\"any\",thresh=numColumns-40)\n",
    "print(\"Before:\",X_train.shape)\n",
    "print(\"After:\",X_trainDroppedRows.shape)\n",
    "\n",
    "tolPercMissing = 0.08\n",
    "#tolerate at most 8% null values in a row\n",
    "X_trainDroppedRows = X_train.dropna(axis=0,how=\"any\",thresh=numColumns-numColumns*tolPercMissing)\n",
    "X_trainDroppedRows = X_trainDroppedRows.fillna(0)\n",
    "print(\"Before:\",X_train.shape)\n",
    "print(\"After:\",X_trainDroppedRows.shape)\n",
    "remainingIndeciesTrain = X_trainDroppedRows.index\n",
    "y_trainDroppedRows = y_train.iloc[remainingIndeciesTrain]\n",
    "\n",
    "\n",
    "X_testDroppedRows = X_test.dropna(axis=0,how=\"any\",thresh=numColumns-numColumns*tolPercMissing)\n",
    "remainingIndeciesTest = X_testDroppedRows.index\n",
    "y_testDroppedRows = y_test.iloc[remainingIndeciesTest]\n",
    "X_testDroppedRows = X_testDroppedRows.fillna(0)\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_trainDroppedRows,y_trainDroppedRows)\n",
    "print(\"Accuracy with dropped rows with null values:\",log_reg.score(X_testDroppedRows,y_testDroppedRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (56000, 784)\n",
      "After: (56000, 0)\n",
      "Before: (56000, 784)\n",
      "After: (56000, 576)\n",
      "Accuracy with dropped columns with null values: 0.9125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxschallwig/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#drop columns with null values\n",
    "#ktolerate at most 1000 null values in a column\n",
    "numRows = X_train.shape[0]\n",
    "#tolerate at most 2000 null values in a column\n",
    "X_trainDroppedColumns = X_train.dropna(axis=1,how=\"any\",thresh=numRows-2000)\n",
    "print(\"Before:\",X_train.shape)\n",
    "print(\"After:\",X_trainDroppedColumns.shape)\n",
    "\n",
    "tolPercMissing = 0.049\n",
    "#tolerate at most 4.9% null values in a column \n",
    "#(in this case we know that at most 5% of each column is null)\n",
    "X_trainDroppedColumns = X_train.dropna(axis=1,how=\"any\",thresh=numRows-numRows*tolPercMissing)\n",
    "print(\"Before:\",X_train.shape)\n",
    "print(\"After:\",X_trainDroppedColumns.shape)\n",
    "remainingColumns = X_trainDroppedColumns.columns\n",
    "\n",
    "X_trainDroppedColumns = X_trainDroppedColumns.fillna(0)\n",
    "X_testDroppedColumns = X_test[remainingColumns].fillna(0)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_trainDroppedColumns,y_train)\n",
    "print(\"Accuracy with dropped columns with null values:\",log_reg.score(X_testDroppedColumns,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with mean imputed null values: 0.9175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxschallwig/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#Simple Imputing \n",
    "from sklearn.impute import SimpleImputer\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\")#\"mean\",\"median\",\"most_frequent\",\"constant\"\n",
    "#                        fill_value=0.5)\n",
    "\n",
    "X_train_mean_imputed = mean_imputer.fit_transform(X_train)\n",
    "X_test_mean_imputed = mean_imputer.transform(X_test)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_mean_imputed,y_train)\n",
    "print(\"Accuracy with mean imputed null values:\",log_reg.score(X_test_mean_imputed,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Imputing\n",
    "from sklearn.impute import KNNImputer\n",
    "#Find N nearest samples (if features that neither is missing are close)\n",
    "#Using 1 because large sample size and otherwise finding higher number of nearest neighbours would take long\n",
    "knn_imputer = KNNImputer(n_neighbors=1)#different distance definions (documentation)\n",
    "\n",
    "X_train_knn_imputed = knn_imputer.fit_transform(X_train)\n",
    "X_test_knn_imputed = knn_imputer.transform(X_test)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_knn_imputed,y_train)\n",
    "print(\"Accuracy with knn imputed null values:\",log_reg.score(X_test_knn_imputed,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom imputing based on knowledge of problem\n",
    "def getNearestPixels(row,column,maxRow,maxColumn):\n",
    "    nearestpixels = []\n",
    "    #left\n",
    "    if column>0:\n",
    "        nearestpixels.append([row,column-1])\n",
    "        #left diagonal down\n",
    "        if row<maxRow:\n",
    "            nearestpixels.append([row+1,column-1])\n",
    "        #left diagonal up\n",
    "        if row>0:\n",
    "            nearestpixels.append([row-1,column-1])\n",
    "    #right\n",
    "    if column<maxColumn:\n",
    "        nearestpixels.append([row,column+1])\n",
    "        #right diagonal down\n",
    "        if row<maxRow:\n",
    "            nearestpixels.append([row+1,column+1])\n",
    "        #right diagonal up\n",
    "        if row>0:\n",
    "            nearestpixels.append([row-1,column+1])\n",
    "    \n",
    "    #up\n",
    "    if row>0:\n",
    "        nearestpixels.append([row-1,column])\n",
    "        \n",
    "    #down\n",
    "    if row<maxRow:\n",
    "        nearestpixels.append([row+1,column])\n",
    "    return nearestpixels\n",
    "\n",
    "def imputeRow(pixels):\n",
    "    reshaped = np.reshape(pixels.values,(int(np.sqrt(pixels.size)),-1))\n",
    "    maxRow = reshaped.shape[0]\n",
    "    maxColumn = reshaped.shape[1]\n",
    "    for row in range(maxRow):\n",
    "        for column in range(maxColumn):\n",
    "            if pd.isnull(reshaped[row,column]):\n",
    "                #if a pixel is null find the indecies of the surrounding pixels\n",
    "                nearestpixels = getNearestPixels(row,column,maxRow-1,maxColumn-1)\n",
    "                sur = []\n",
    "                #get values of surrounding pixels\n",
    "                for inds in nearestpixels:\n",
    "                    sur.append(reshaped[inds[0],inds[1]])\n",
    "                #Setting value of null to mean of surrounding pixels\n",
    "                reshaped[row,column] = np.nanmean(sur)\n",
    "                \n",
    "    return reshaped.reshape(1,pixels.size)[0]\n",
    "                \n",
    "    \n",
    "#Applying function to each row\n",
    "#Transforming data since each function is return an array, sklearn doesn't like the returned dataformat\n",
    "#so we put it back into standard nested list/array format\n",
    "X_train_custom_impute = [x for x in X_train.apply(lambda row: imputeRow(row),axis=1).values]\n",
    "X_test_custom_impute = [x for x in X_test.apply(lambda row: imputeRow(row),axis=1).values]\n",
    "y_train_custom_impute = y_train.values\n",
    "y_test_custom_impute = y_test.values\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_custom_impute,y_train_custom_impute)\n",
    "print(\"Accuracy with custom imputed null values:\",log_reg.score(X_test_custom_impute,y_test_custom_impute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
